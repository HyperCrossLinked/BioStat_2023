---
title: "automatization_notebook_02"
output: word_document
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(stringi)
library(dplyr)


```

# Чтение данных

В вашем варианте нужно использовать датасет food.

```{r}
data <- as_tibble(read_delim("data/raw/food.csv", delim = NULL, quote = "\"", na = c("", "NA"), skip = 0, n_max = Inf, col_names = TRUE))

```

# Выведите общее описание данных

```{r}

# summary(data)
 glimpse(data)
# psych::describe(data)

```

# Очистка данных

1)  Уберите переменные, в которых пропущенных значений больше 20% или уберите субъектов со слишком большим количеством пропущенных значений. Или совместите оба варианта. Напишите обоснование, почему вы выбрали тот или иной вариант:

**В датасете про еду вообще вроде как нет пропусков. Но выбор типа отсеивания зависит от природы данных и главное от вопроса задаваемого данным. Например если бы рассматривали медицинский датасет и предсказывали летальность, то переменную с большим числом пропусков требуется исключить т.к. вероятно это информативная цензура и эта переменная может вылезти как значимый фактор. Но в реальном мире мы его не могли бы использовать т.к. мало наблюдений. К датасету еда трудно придумать вопрос. Если мы будем стравнивать средние значения, то я бы исключил продукты про состав которых мало что известно**

2)  Переименуйте переменные в человекочитаемый вид (что делать с пробелами в названиях?);
3)  В соответствии с описанием данных приведите переменные к нужному типу (numeric или factor);
4)  Отсортируйте данные по возрасту по убыванию;
5)  Сохраните в файл outliers.csv субъектов, которые являются выбросами (например, по правилу трёх сигм) --- это необязательное задание со звёздочкой;
6)  Отфильтруйте датасет так, чтобы остались только Rice и Cookie (переменная Category и есть группирующая);
7)  Присвойте получившийся датасет переменной "cleaned_data".

```{r}
fdata <- data %>% 
  # Выбираем столбцы по числу пропусков а затем строки
  select(which(colMeans(is.na(.)) <= 0.2)) %>% 
  filter(rowSums(is.na(.)) / ncol(.) <= 0.2) %>% 
  
  # Переиминуем все переменные, заменим в названии дата пробелы и прочее на нижний прочерк
  rename_with(function(x) x %>% stri_replace_all_regex(c("Data.", "\\.", " ", "-"), c("", "_", "_", ""), vectorize_all = F)) %>% 
  
  # Вроде как тут только категория факторная переменная, остальные нумерик
  # `Nutrient Data Bank Number` - уникальный номер, является ли он фактором?
  # Пр идее да, как в моделях со случайным интерцептом
  # Но далее будет не красиво если считаем это категориями
  mutate(across(c(`Category`), function(x) as.factor(x))) %>% 
  mutate(across(c(`Description`), function(x) as.character(x))) %>% 
  
  # Сортируем по убыванию
  # Из задания не понятно по какой переменной
  arrange(desc(`Sugar_Total`)) 

# Сохраняем выбросы в датасет как строки, значение переменных которых отклоняется от среднего значения по всем строкам более чем на 3 сигмы
outliers <- fdata %>% 
  filter(if_any(where(is.numeric), function(x) abs(x - mean(x)) > 3 * sd(x)))

# Сохраняем выбросы в файл
outliers %>% 
  write_delim("data/outliers.csv", delim = ",", na = "NA")

# Cохраняем только рис и печеньки
cleaned_data <- fdata %>% 
  filter(`Category` %in% c('Rice', 'Cookie'))

# Смотрим что получилось
glimpse(fdata)

```

# Сколько осталось переменных?

```{r}

ncol(cleaned_data)

```

# Сколько осталось случаев?

```{r}

nrow(cleaned_data)

```

# Есть ли в данных идентичные строки?

```{r}

any(duplicated(fdata))

```

# Сколько всего переменных с пропущенными значениями в данных и сколько пропущенных точек в каждой такой переменной?

```{r}

# Сохраним в фрейм на_каунт сумму значений являющихся na применённое ко всем переменным
na_count <- cleaned_data %>% 
  summarise_all(~ sum(is.na(.)))

# print(na_count)

# Приведём фрэйм в вектор и проссумируем все значения, получи сколько всего пропусков 
n_vars_with_na <- na_count %>% 
  unlist %>% 
  sum()
print(n_vars_with_na)

```

# Описательные статистики

## Количественные переменные

1)  Рассчитайте для всех количественных переменных для каждой группы (Category):

1.1) Количество значений;

1.2) Количество пропущенных значений;

1.3) Среднее;

1.4) Медиану;

1.5) Стандартное отклонение;

1.6) 25% квантиль и 75% квантиль;

1.7) Интерквартильный размах;

1.8) Минимум;

1.9) Максимум;

1.10) 95% ДИ для среднего - задание со звёздочкой.

```{r}

 statistics <-  list(
    `_Количество значений` = ~sum(!is.na(.)) %>% as.character,
    `_Количество пропущенных значений` = ~sum(is.na(.)) %>% as.character,
    `_Ср. знач.` = ~ifelse(sum(!is.na(.x)) == 0, "Н/П*", mean(.x, na.rm = TRUE) %>% round(2) %>% as.character()),
    `_Станд. отклон.` = ~ifelse(sum(!is.na(.x)) < 3, "Н/П*", sd(.x, na.rm = TRUE) %>% round(2) %>% as.character()),
    
    # С ходу не понял как знаения t распределения посчитать, поэтому проще посчитаем
    # По хорошему значения не нормальны, поэтому бутстрапом нужно ДИ искарть
    `_95% ДИ для среднего верх` = ~(mean(.x, na.rm = TRUE) + 1.96*(sd(.x, na.rm = TRUE)/sqrt(sum(!is.na(.))))) %>% round(2) %>% as.character(),
    `_95% ДИ для среднего низ` = ~(mean(.x, na.rm = TRUE) - 1.96*(sd(.x, na.rm = TRUE)/sqrt(sum(!is.na(.))))) %>% round(2) %>% as.character(),
    `_мин. - макс.` = ~ifelse(sum(!is.na(.x)) == 0, "Н/П*", paste0(min(.x, na.rm = TRUE) %>% round(2), " - ", max(.x, na.rm = TRUE) %>% round(2))),
    `_Медиана` = ~ifelse(sum(!is.na(.x)) == 0, "Н/П*", median(.x, na.rm = TRUE) %>% round(2) %>% as.character()),
    `_Q1 - Q3` = ~ifelse(sum(!is.na(.x)) == 0, "Н/П*", paste0(quantile(.x, 0.25, na.rm = TRUE) %>% round(2), " - ", quantile(.x, 0.75, na.rm = TRUE) %>% round(2)))
  )

cleaned_data %>% 
  select(Category, where(is.numeric)) %>% 
  group_by(Category) %>% 
  summarise(across(where(is.numeric), statistics)) %>% 
  pivot_longer(!Category) %>% 
  separate(name, into = c("Переменная", "Стаитистика"), sep = "__") %>% 
  rename(`Значение` = value)

```

## Категориальные переменные

1)  Рассчитайте для всех категориальных переменных для каждой группы (Category):

1.1) Абсолютное количество;

1.2) Относительное количество внутри группы;

1.3) 95% ДИ для доли внутри группы - задание со звёздочкой.

**Не ясно что требуется. У нас одна категориальная переменная. Получается нет категориальных кроме групповой переменной. Относительная доля риса в группе риса 100%**

```{r}

# Расчет абсолютного количества
cleaned_data %>% 
  group_by(Category) %>% 
  count(name = "Absolute Count")

# Расчет относительного количества внутри группы
cleaned_data %>% 
  group_by(Category) %>% 
  summarise(n = n()) %>% 
  mutate(Relative_Count = n / sum(n))


```

# Визуализация

## Количественные переменные

1)  Для каждой количественной переменной сделайте боксплоты по группам. Расположите их либо на отдельных рисунках, либо на одном, но читаемо;

2)  Наложите на боксплоты beeplots - задание со звёздочкой.

3)  Раскрасьте боксплоты с помощью библиотеки RColorBrewer.

```{r}

for (col in cleaned_data %>% select_if(is.numeric)) {
  #print(col)
  print(ggplot(cleaned_data, aes(x = Category, y = col, color = Category, fill = Category)) +
    geom_boxplot() + 
    scale_colour_brewer(palette = "Set1") +
    scale_fill_brewer(palette = "Set1")
  )
}  

```

## Категориальные переменные

1)  Сделайте подходящие визуализации категориальных переменных. Обоснуйте, почему выбрали именно этот тип.

```{r}

for (name in names(cleaned_data %>% select_if(is.factor))) {
  print(
    ggplot(cleaned_data, aes_string(x = "1", fill = name)) +
    geom_bar(width = 1) +
    coord_polar(theta = "y") +
    scale_fill_brewer(palette = "Set1") +
    labs(x = NULL, fill = name) +
    theme_void() +
    theme(legend.position = "right")
  )
}

```

# Статистические оценки

## Проверка на нормальность

1)  Оцените каждую переменную на соответствие нормальному распределению с помощью теста Шапиро-Уилка. Какие из переменных являются нормальными и как как вы это поняли?

**нормальными являются те переменные, где p-value меньше 0.05. В данном случае нормальных переменных нет. Для крупных выборок ИМХО логичнее использовать не Р-значение а значение статистики Шапиро-Уилка, т.к. в конечном итоге важно не значимость отклонения от нормальности а то, на сколько они отличаются от нормального распределения (любые огромные полученные на реальных данных будут не ннормальны, мощьность теста избыточна). Я бы взял граничное значение W < 0.9**

```{r}

numeric_vars <- cleaned_data %>% select_if(is.numeric)

shapiro_results_list <- lapply(numeric_vars, shapiro.test)

shapiro_results <- data.frame(
    variable = names(shapiro_results_list),
    W = sapply(shapiro_results_list, function(x) x$statistic),
    p_value = sapply(shapiro_results_list, function(x) x$p.value)
)

shapiro_results <- shapiro_results %>% 
    mutate(`is_normal` = if_else(p_value > 0.05, "yes", "no"))

print(shapiro_results)

```

2)  Постройте для каждой количественной переменной QQ-плот. Отличаются ли выводы от теста Шапиро-Уилка? Какой метод вы бы предпочли и почему?

```{r}

cols <- colnames(cleaned_data %>% select_if(is.numeric))

for (col in cols) {
  qqnorm(cleaned_data[[col]], main = col)
  qqline(cleaned_data[[col]], col = "red")
}

```

3)  Ниже напишите, какие ещё методы проверки на нормальность вы знаете и какие у них есть ограничения.

**Этих методов огромное множесство, даже ГОСТ на это видел. Их можно условно разделить на методы которые проверяют ассиметрию и эксцесс и методы сравнивающие распределение в целом с нормальным распределением. На практике часто видно не нормальные данные если, условно говоря Mean - 1,5\*SD перекрывается с 0, при условии что значения ниже нуля не возможны.**

## Сравнение групп

1)  Сравните группы (переменная **Category**) по каждой переменной (как количественной, так и категориальной). Для каждой переменной выберите нужный критерий и кратко обоснуйте его выбор в комментариях.

**Далее я сломался, попробую сделать к окончательному дедлайну**

```{r}


```

# Далее идут **необязательные** дополнительные задания, которые могут принести вам дополнительные баллы в том числе в случае ошибок в предыдущих

## Корреляционный анализ

1)  Создайте корреляционную матрицу с визуализацией и поправкой на множественные сравнения. Объясните, когда лучше использовать корреляционные матрицы и в чём минусы и плюсы корреляционных исследований.

```{r}



```

## Моделирование

1)  Постройте регрессионную модель для переменной **Category**. Опишите процесс построения

```{r}



```
